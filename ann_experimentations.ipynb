{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import KFold\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the required x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"data/X_train.csv\")\n",
    "y_train = pd.read_csv(\"data/y_train.csv\").squeeze()  # Convert to Series\n",
    "X_test = pd.read_csv(\"data/X_test.csv\")\n",
    "y_test = pd.read_csv(\"data/y_test.csv\").squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to NumPy array before converting to tensor\n",
    "X_train_tensor = torch.tensor(X_train.to_numpy(), dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test.to_numpy(), dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SLP model\n",
    "class SingleLayerPerceptron(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SingleLayerPerceptron, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, 1)  # Single layer\n",
    "        self.sigmoid = nn.Sigmoid()  # Activation function\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X_train.shape[1]  # Number of features\n",
    "model = SingleLayerPerceptron(input_size)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.5938\n",
      "Epoch 2/20, Loss: 0.5097\n",
      "Epoch 3/20, Loss: 0.4797\n",
      "Epoch 4/20, Loss: 0.4640\n",
      "Epoch 5/20, Loss: 0.4548\n",
      "Epoch 6/20, Loss: 0.4481\n",
      "Epoch 7/20, Loss: 0.4432\n",
      "Epoch 8/20, Loss: 0.4402\n",
      "Epoch 9/20, Loss: 0.4391\n",
      "Epoch 10/20, Loss: 0.4362\n",
      "Epoch 11/20, Loss: 0.4356\n",
      "Epoch 12/20, Loss: 0.4343\n",
      "Epoch 13/20, Loss: 0.4337\n",
      "Epoch 14/20, Loss: 0.4337\n",
      "Epoch 15/20, Loss: 0.4338\n",
      "Epoch 16/20, Loss: 0.4326\n",
      "Epoch 17/20, Loss: 0.4319\n",
      "Epoch 18/20, Loss: 0.4307\n",
      "Epoch 19/20, Loss: 0.4322\n",
      "Epoch 20/20, Loss: 0.4315\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        y_batch = y_batch.float().unsqueeze(1)  # Reshape for BCELoss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7927\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        predicted = (outputs > 0.5).float()  # Convert probabilities to 0 or 1\n",
    "        correct += (predicted.squeeze() == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Building a mlp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MLP Model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "\n",
    "        # Create hidden layers\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.ReLU())  # Activation function\n",
    "            prev_size = hidden_size\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        layers.append(nn.Sigmoid())  # Sigmoid for binary classification\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NumPy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.from_numpy(X_train.to_numpy()).float()\n",
    "y_train_tensor = torch.from_numpy(y_train.to_numpy()).long()\n",
    "X_test_tensor = torch.from_numpy(X_test.to_numpy()).float()\n",
    "y_test_tensor = torch.from_numpy(y_test.to_numpy()).long()\n",
    "\n",
    "# Define Hyperparameters\n",
    "hidden_layer_configs = [[32], [64, 32], [128, 64, 32]]  # Different layer combinations\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "batch_sizes = [16, 32, 64]\n",
    "num_epochs = 30\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)  # 5-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(hidden_sizes, learning_rate, batch_size):\n",
    "    dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = MLP(input_size=X_train_tensor.shape[1], hidden_sizes=hidden_sizes, output_size=1)\n",
    "    criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X).squeeze()\n",
    "            loss = criterion(outputs, batch_y.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test_tensor).squeeze()\n",
    "        test_predictions = (test_outputs >= 0.5).long()\n",
    "        accuracy = (test_predictions == y_test_tensor).float().mean().item()\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: [32], LR: 0.01, Batch: 16, Acc: 0.7883\n",
      "Config: [32], LR: 0.01, Batch: 32, Acc: 0.7891\n",
      "Config: [32], LR: 0.01, Batch: 64, Acc: 0.7887\n",
      "Config: [32], LR: 0.001, Batch: 16, Acc: 0.7923\n",
      "Config: [32], LR: 0.001, Batch: 32, Acc: 0.7913\n",
      "Config: [32], LR: 0.001, Batch: 64, Acc: 0.7911\n",
      "Config: [32], LR: 0.0001, Batch: 16, Acc: 0.7892\n",
      "Config: [32], LR: 0.0001, Batch: 32, Acc: 0.7885\n",
      "Config: [32], LR: 0.0001, Batch: 64, Acc: 0.7780\n",
      "Config: [64, 32], LR: 0.01, Batch: 16, Acc: 0.7885\n",
      "Config: [64, 32], LR: 0.01, Batch: 32, Acc: 0.7874\n",
      "Config: [64, 32], LR: 0.01, Batch: 64, Acc: 0.7873\n",
      "Config: [64, 32], LR: 0.001, Batch: 16, Acc: 0.7859\n",
      "Config: [64, 32], LR: 0.001, Batch: 32, Acc: 0.7883\n",
      "Config: [64, 32], LR: 0.001, Batch: 64, Acc: 0.7879\n",
      "Config: [64, 32], LR: 0.0001, Batch: 16, Acc: 0.7913\n",
      "Config: [64, 32], LR: 0.0001, Batch: 32, Acc: 0.7911\n",
      "Config: [64, 32], LR: 0.0001, Batch: 64, Acc: 0.7927\n",
      "Config: [128, 64, 32], LR: 0.01, Batch: 16, Acc: 0.7867\n",
      "Config: [128, 64, 32], LR: 0.01, Batch: 32, Acc: 0.7882\n",
      "Config: [128, 64, 32], LR: 0.01, Batch: 64, Acc: 0.7892\n",
      "Config: [128, 64, 32], LR: 0.001, Batch: 16, Acc: 0.7825\n",
      "Config: [128, 64, 32], LR: 0.001, Batch: 32, Acc: 0.7873\n",
      "Config: [128, 64, 32], LR: 0.001, Batch: 64, Acc: 0.7879\n",
      "Config: [128, 64, 32], LR: 0.0001, Batch: 16, Acc: 0.7901\n",
      "Config: [128, 64, 32], LR: 0.0001, Batch: 32, Acc: 0.7919\n",
      "Config: [128, 64, 32], LR: 0.0001, Batch: 64, Acc: 0.7903\n",
      "Best Config: ([64, 32], 0.0001, 64) with Accuracy: 0.7927\n"
     ]
    }
   ],
   "source": [
    "best_config = None\n",
    "best_accuracy = 0\n",
    "\n",
    "for hidden_sizes in hidden_layer_configs:\n",
    "    for lr in learning_rates:\n",
    "        for batch_size in batch_sizes:\n",
    "            accuracies = []\n",
    "            for train_idx, val_idx in kf.split(X_train_tensor):\n",
    "                X_train_fold, X_val_fold = X_train_tensor[train_idx], X_train_tensor[val_idx]\n",
    "                y_train_fold, y_val_fold = y_train_tensor[train_idx], y_train_tensor[val_idx]\n",
    "\n",
    "                acc = train_and_evaluate(hidden_sizes, lr, batch_size)\n",
    "                accuracies.append(acc)\n",
    "\n",
    "            mean_acc = np.mean(accuracies)\n",
    "            print(f\"Config: {hidden_sizes}, LR: {lr}, Batch: {batch_size}, Acc: {mean_acc:.4f}\")\n",
    "\n",
    "            if mean_acc > best_accuracy:\n",
    "                best_accuracy = mean_acc\n",
    "                best_config = (hidden_sizes, lr, batch_size)\n",
    "\n",
    "print(f\"Best Config: {best_config} with Accuracy: {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy: 0.7943\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the best model\n",
    "best_hidden_sizes, best_lr, best_batch_size = best_config\n",
    "best_accuracy = train_and_evaluate(best_hidden_sizes, best_lr, best_batch_size)\n",
    "print(f\"Best Accuracy: {best_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[1421  332]\n",
      " [ 410  837]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.81      0.79      1753\n",
      "           1       0.72      0.67      0.69      1247\n",
      "\n",
      "    accuracy                           0.75      3000\n",
      "   macro avg       0.75      0.74      0.74      3000\n",
      "weighted avg       0.75      0.75      0.75      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#confusion matrix and classification report\n",
    "y_pred = model(X_test_tensor).squeeze()\n",
    "y_pred = (y_pred >= 0.5).long()\n",
    "confusion = confusion_matrix(y_test_tensor, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)\n",
    "classification_report = classification_report(y_test_tensor, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model has:\n",
    "Hidden Layers: [64, 32]\n",
    "Learning Rate: 0.0001\n",
    "Batch Size: 64\n",
    "Accuracy: 0.7927 (79.27%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A two-layer MLP (64, 32 neurons) worked better than simpler or deeper models.\n",
    "A lower learning rate (0.0001) gave better stability and accuracy.\n",
    "A batch size of 64 was optimal for efficient training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Saving the best model\n",
    "torch.save(model.state_dict(), 'best_model.pth')\n",
    "print(\"Model saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
