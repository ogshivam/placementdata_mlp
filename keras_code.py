# -*- coding: utf-8 -*-
"""keras.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UZknKmj6oVkZYlRDw_9DbEHHyWNnafgk

## Importing Required Libraries
"""

import pandas as pd
import numpy as np
from tensorflow import keras
from tensorflow.keras import layers, models
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold

"""# Loading the data"""

# Load the data
X_train = pd.read_csv("data/X_train.csv")
y_train = pd.read_csv("data/y_train.csv").squeeze()
X_test = pd.read_csv("data/X_test.csv")
y_test = pd.read_csv("data/y_test.csv").squeeze()

"""## Building the perceptron"""

# Single Layer Perceptron Model
def create_perceptron(input_size):
    model = models.Sequential([
        layers.Dense(1, input_shape=(input_size,), activation='sigmoid')
    ])

    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.01),
        loss='binary_crossentropy',
        metrics=['accuracy']
    )
    return model

# Traing the perceptron model

# Train Single Layer Perceptron
input_size = X_train.shape[1]
perceptron = create_perceptron(input_size)
perceptron.fit(
    X_train,
    y_train,
    epochs=20,
    batch_size=32,
    validation_data=(X_test, y_test)
)

# Evaluate Perceptron
perceptron_scores = perceptron.evaluate(X_test, y_test)
print(f"Perceptron Test Accuracy: {perceptron_scores[1]:.4f}")

"""## Building the MLP Model"""

# MLP Model Class
def create_mlp(input_size, hidden_sizes):
    model = models.Sequential()

    # Input layer
    model.add(layers.Dense(hidden_sizes[0], input_shape=(input_size,), activation='relu'))

    # Hidden layers
    for hidden_size in hidden_sizes[1:]:
        model.add(layers.Dense(hidden_size, activation='relu'))

    # Output layer
    model.add(layers.Dense(1, activation='sigmoid'))

    return model

# Hyperparameter Search
hidden_layer_configs = [[32], [64, 32], [128, 64, 32]]
learning_rates = [0.01, 0.001, 0.0001]
batch_sizes = [16, 32, 64]
num_epochs = 30
kf = KFold(n_splits=5, shuffle=True, random_state=42)

def train_and_evaluate(hidden_sizes, learning_rate, batch_size, X_train, y_train, X_test, y_test):
    model = create_mlp(X_train.shape[1], hidden_sizes)
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),
        loss='binary_crossentropy',
        metrics=['accuracy']
    )

    history = model.fit(
        X_train, y_train,
        epochs=num_epochs,
        batch_size=batch_size,
        validation_split=0.2,
        verbose=0
    )

    _, accuracy = model.evaluate(X_test, y_test, verbose=0)
    return accuracy

# Grid Search with Cross-Validation
best_config = None
best_accuracy = 0

for hidden_sizes in hidden_layer_configs:# Load the data
    for lr in learning_rates:
        for batch_size in batch_sizes:
            accuracies = []

            for train_idx, val_idx in kf.split(X_train):
                X_train_fold = X_train.iloc[train_idx]
                X_val_fold = X_train.iloc[val_idx]
                y_train_fold = y_train.iloc[train_idx]
                y_val_fold = y_train.iloc[val_idx]

                acc = train_and_evaluate(
                    hidden_sizes, lr, batch_size,
                    X_train_fold, y_train_fold,
                    X_val_fold, y_val_fold
                )
                accuracies.append(acc)

            mean_acc = np.mean(accuracies)
            print(f"Config: {hidden_sizes}, LR: {lr}, Batch: {batch_size}, Acc: {mean_acc:.4f}")

            if mean_acc > best_accuracy:
                best_accuracy = mean_acc
                best_config = (hidden_sizes, lr, batch_size)

print(f"\nBest Config: {best_config} with Accuracy: {best_accuracy:.4f}")

"""The best model has:
Hidden Layers: [64, 32]
Learning Rate: 0.01
Batch Size: 64
Accuracy: 0.80329 (80.39%)
"""

# Train Final Model with Best Configuration
best_hidden_sizes, best_lr, best_batch_size = best_config
final_model = create_mlp(X_train.shape[1], best_hidden_sizes)
final_model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=best_lr),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

final_model.fit(
    X_train, y_train,
    epochs=num_epochs,
    batch_size=best_batch_size,
    validation_data=(X_test, y_test)
)

# Final Evaluation
y_pred = (final_model.predict(X_test) >= 0.5).astype(int)
conf_matrix =  confusion_matrix(y_test, y_pred)
confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:")
print(conf_matrix)
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

"""A two-layer MLP (64, 32 neurons) worked better than simpler or deeper models.
A learning rate (0.01) gave better stability and accuracy.
A batch size of 64 was optimal for efficient training.
"""

# Save the model
final_model.save('best_model.h5')
print("\nModel saved successfully!")

